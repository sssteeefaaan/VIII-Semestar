{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EAF1A2SqhMH"
      },
      "source": [
        "# Januar 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "443mcksibRm7"
      },
      "outputs": [],
      "source": [
        "%%writefile jan_2022.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__host__ void initialize_vector(int** A, int n);\n",
        "__host__ void operate_on_GPU(int* A, int* B, int n);\n",
        "__global__ void kernel(int* A, int* B, int n);\n",
        "__host__ bool check_result(int*A, int*B, int n);\n",
        "__host__ void print_vector(const char* lbl, int* A, int n);\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    int *A, *B;\n",
        "    \n",
        "    for(int n = 255; n < 3245; n += 3)\n",
        "    {\n",
        "      initialize_vector(&A, n);\n",
        "      B = (int*) malloc(sizeof(int) * (n - 2));\n",
        "\n",
        "      operate_on_GPU(A, B, n);\n",
        "\n",
        "      if(check_result(A, B, n))\n",
        "        cout << n << \" Correct!\" << endl;\n",
        "      else\n",
        "        cout << n << \" False!\" << endl;\n",
        "\n",
        "      free(B);\n",
        "      free(A);\n",
        "    }\n",
        " \n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "__host__ void initialize_vector(int** A, int n)\n",
        "{\n",
        "    *A = (int*)malloc(sizeof(int) * n);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      (*A)[i] = i + 1;\n",
        "}\n",
        "\n",
        "__host__ void operate_on_GPU(int* A, int* B, int n)\n",
        "{\n",
        "    int* dev_A, *dev_B;\n",
        "    size_t full = sizeof(int) * n,\n",
        "            part = sizeof(int) * (n - 2);\n",
        "\n",
        "    cudaError_t err;\n",
        " \n",
        "    err = cudaMalloc(&dev_A, full);\n",
        "    if(err)\n",
        "      cout << \"1A \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMalloc(&dev_B, part);\n",
        "    if(err)\n",
        "          cout << \"1B \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMemcpy(dev_A, A, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2 \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    kernel<<<min(256, n / BLOCK_SIZE + 1), BLOCK_SIZE>>>(dev_A, dev_B, n);\n",
        "\n",
        "    err = cudaMemcpy(B, dev_B, part, cudaMemcpyDeviceToHost);\n",
        "    if(err)\n",
        "          cout << \"3 \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_A);\n",
        "}\n",
        "\n",
        "__global__ void kernel(int* A, int* B, int n)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    __shared__ int sh[BLOCK_SIZE];\n",
        " \n",
        "    while(tid < n - 1)\n",
        "    {\n",
        "        sh[threadIdx.x] = A[tid + 1];\n",
        "        __syncthreads();\n",
        "     \n",
        "        if(tid < n - 2)\n",
        "        {\n",
        "          if(threadIdx.x == 0)\n",
        "            B[tid] = (A[tid] + sh[threadIdx.x] + sh[threadIdx.x + 1]) / 3;\n",
        "          else if(threadIdx.x == (blockDim.x - 1))\n",
        "            B[tid] = (sh[threadIdx.x - 1] + sh[threadIdx.x] + A[tid + 2]) / 3;\n",
        "          else\n",
        "            B[tid] = (sh[threadIdx.x - 1] + sh[threadIdx.x] + sh[threadIdx.x + 1]) / 3;\n",
        "        }\n",
        "     \n",
        "        __syncthreads();\n",
        "     \n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__ bool check_result(int*A, int*B, int n)\n",
        "{\n",
        "    bool c = true;\n",
        "    for(int i = 0; c && i < n - 2; i++)\n",
        "      c = B[i] == ((A[i] + A[i + 1] + A[i + 2]) / 3);\n",
        "    return c;\n",
        "}\n",
        "\n",
        "__host__ void print_vector(const char* lbl, int* A, int n)\n",
        "{\n",
        "    cout << lbl << \" = |\\t\";\n",
        "    for(int i = 0; i < n; (cout << A[i++] << \"\\t\"));\n",
        "    cout << \"|\\n\";\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSGfILV7i0L1"
      },
      "outputs": [],
      "source": [
        "filepath = \"jan_2022.cu\"  #@param { type: \"string\" }\n",
        "compiled_filepath = \"jan_2022\"  #@param { type: \"string\" }\n",
        "\n",
        "!nvcc -arch=sm_37 -gencode=arch=compute_37,code=sm_37 $filepath -o $compiled_filepath\n",
        "\n",
        "argv = \"\" #@param [] { allow-input: true }\n",
        "\n",
        "!./$compiled_filepath $argv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwMuu9sHqnW9"
      },
      "source": [
        "# Jun 2 2021\n",
        "# Ovo je takav overkill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_n4SMhQqscY"
      },
      "outputs": [],
      "source": [
        "%%writefile jun2_2021.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "__host__ void initialize_vector(int** A, int n);\n",
        "__host__ void operate_on_GPU(int* A, int* B, int* v, int n, int m);\n",
        "__global__ void kernel_sum(int* A, int* B, int* v, int n, int m);\n",
        "__global__ void kernel_reduce(int* v, int n, int m, int el_numb);\n",
        "__host__ bool check_result(int*A, int*B, int *v, int n, int m);\n",
        "__host__ void print_vector(const char* lbl, int* A, int n);\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    int n = 123;\n",
        "    for(int m = 1; m < 65432; m += 19)\n",
        "    {\n",
        "      int *A, *B, *v;\n",
        "  \n",
        "      initialize_vector(&A, n * m);\n",
        "      initialize_vector(&B, n * m);\n",
        "      v = (int*) malloc(sizeof(int) * n);\n",
        "\n",
        "      operate_on_GPU(A, B, v, n, m);\n",
        "\n",
        "      if(check_result(A, B, v, n, m))\n",
        "        cout << \"(\" << n << \", \" << m << \") Correct!\" << endl;\n",
        "      else\n",
        "        cout << \"(\" << n << \", \" << m << \") False!\" << endl;\n",
        "\n",
        "      free(v);\n",
        "      free(B);\n",
        "      free(A);\n",
        "    }\n",
        " \n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "__host__ void initialize_vector(int** A, int n)\n",
        "{\n",
        "    *A = (int*)malloc(sizeof(int) * n);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      (*A)[i] = i + 1;\n",
        "}\n",
        "\n",
        "__host__ void operate_on_GPU(int* A, int* B, int* v, int n, int m)\n",
        "{\n",
        "    int* dev_A, *dev_B, *dev_v;\n",
        " \n",
        "    dim3 grid_size(min(256, (m / BLOCK_SIZE) + 1), min(256, (n / BLOCK_SIZE) + 1)),\n",
        "        block_size(BLOCK_SIZE, BLOCK_SIZE);\n",
        " \n",
        "    size_t full = sizeof(int) * n * m,\n",
        "            part = sizeof(int) * n * grid_size.x;\n",
        "\n",
        "    cudaError_t err;\n",
        " \n",
        "    err = cudaMalloc(&dev_A, full);\n",
        "    if(err)\n",
        "      cout << \"1A \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMalloc(&dev_B, full);\n",
        "    if(err)\n",
        "          cout << \"1B \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMalloc(&dev_v, part);\n",
        "    if(err)\n",
        "          cout << \"1v \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMemcpy(dev_A, A, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2A \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMemcpy(dev_B, B, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2B \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    kernel_sum<<<grid_size, block_size>>>(dev_A, dev_B, dev_v, n, m);\n",
        "    int temp = grid_size.x;\n",
        "    grid_size.x = 1;\n",
        "    kernel_reduce<<<grid_size, block_size>>>(dev_v, n, temp, m);\n",
        "\n",
        "    err = cudaMemcpy(v, dev_v, sizeof(int) * n, cudaMemcpyDeviceToHost);\n",
        "    if(err)\n",
        "          cout << \"3 \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    cudaFree(dev_v);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_A);\n",
        "}\n",
        "\n",
        "__global__ void kernel_sum(int* A, int* B, int* v, int n, int m)\n",
        "{\n",
        "    int tid_x_initial = 2 * blockIdx.x * blockDim.x + threadIdx.x,\n",
        "      tid_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    __shared__ int sh[BLOCK_SIZE][BLOCK_SIZE];\n",
        " \n",
        "    while(tid_y < n)\n",
        "    {\n",
        "        int tid_x = tid_x_initial;\n",
        "     \n",
        "        sh[threadIdx.y][threadIdx.x] = 0;\n",
        "        while(tid_x < m)\n",
        "        {\n",
        "          sh[threadIdx.y][threadIdx.x] += A[tid_y * m + tid_x] + B[tid_y * m + tid_x];\n",
        "          if(tid_x + blockDim.x < m)\n",
        "            sh[threadIdx.y][threadIdx.x] += A[tid_y * m + tid_x + blockDim.x] + B[tid_y * m + tid_x + blockDim.x];\n",
        "          tid_x += 2 * blockDim.x * gridDim.x;\n",
        "        }\n",
        "     \n",
        "        for(int i = blockDim.x>>1; i > threadIdx.x; i>>=1)\n",
        "        {\n",
        "          __syncthreads();\n",
        "          sh[threadIdx.y][threadIdx.x] += sh[threadIdx.y][threadIdx.x + i];\n",
        "        }\n",
        "     \n",
        "        if(!threadIdx.x)\n",
        "          v[tid_y * gridDim.x + blockIdx.x] = sh[threadIdx.y][0];\n",
        "     \n",
        "        tid_y += blockDim.y * gridDim.y;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void kernel_reduce(int* v, int n, int m, int el_numb)\n",
        "{\n",
        "    int tid_x_initial = 2 * blockIdx.x * blockDim.x + threadIdx.x,\n",
        "      tid_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    __shared__ int sh[BLOCK_SIZE][BLOCK_SIZE];\n",
        " \n",
        "    while(tid_y < n)\n",
        "    {\n",
        "        int tid_x = tid_x_initial;\n",
        "     \n",
        "        sh[threadIdx.y][threadIdx.x] = 0;\n",
        "        while(tid_x < m)\n",
        "        {\n",
        "          sh[threadIdx.y][threadIdx.x] += v[tid_y * m + tid_x];\n",
        "          if(tid_x + blockDim.x < m)\n",
        "            sh[threadIdx.y][threadIdx.x] += v[tid_y * m + tid_x + blockDim.x];\n",
        "          tid_x += 2 * blockDim.x * gridDim.x;\n",
        "        }\n",
        "     \n",
        "        for(int i = blockDim.x >> 1; i > threadIdx.x; i >>= 1)\n",
        "        {\n",
        "          __syncthreads();\n",
        "          sh[threadIdx.y][threadIdx.x] += sh[threadIdx.y][threadIdx.x + i];\n",
        "        }\n",
        "     \n",
        "        if(!threadIdx.x){\n",
        "          v[tid_y * gridDim.x + blockIdx.x] = sh[threadIdx.y][0];\n",
        "          if(gridDim.x == 1)\n",
        "            v[tid_y] /= el_numb;\n",
        "        }\n",
        "     \n",
        "        tid_y += blockDim.y * gridDim.y;\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__ bool check_result(int*A, int*B, int*v, int n, int m)\n",
        "{\n",
        "    bool c = true;\n",
        "    for(int i = 0; c && i < n; i++)\n",
        "    {\n",
        "        int sum = 0;\n",
        "        for(int j = 0; j < m; j++)\n",
        "          sum += A[i * m + j] + B[i * m + j];\n",
        "        c = v[i] == (sum / m);\n",
        "    }\n",
        "    return c;\n",
        "}\n",
        "\n",
        "__host__ void print_vector(const char* lbl, int* A, int n)\n",
        "{\n",
        "    cout << lbl << \" = |\\t\";\n",
        "    for(int i = 0; i < n; (cout << A[i++] << \"\\t\"));\n",
        "    cout << \"|\\n\";\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubAyZAv1s_Mf"
      },
      "outputs": [],
      "source": [
        "filepath = \"jun2_2021.cu\"  #@param { type: \"string\" }\n",
        "compiled_filepath = \"jun2_2021\"  #@param { type: \"string\" }\n",
        "\n",
        "!nvcc -arch=sm_37 -gencode=arch=compute_37,code=sm_37 $filepath -o $compiled_filepath\n",
        "\n",
        "argv = \"\" #@param [] { allow-input: true }\n",
        "\n",
        "!./$compiled_filepath $argv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILZuf9T2cJSJ"
      },
      "source": [
        "# Jun 2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeubNjIacK7_"
      },
      "outputs": [],
      "source": [
        "%%writefile jun_2021.cu\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__host__ void initialize_vector(float** v, int n);\n",
        "__host__ void print_vector(const char* lbl, float* v, int n);\n",
        "__host__ void operate_on_GPU(float* v1, float* v2, float* v3, int n, float* avg, float* deviation);\n",
        "__global__ void vector_sum(float* v1, float* v2, float* v3, int n);\n",
        "__global__ void reduce_sum(float* v, float* sum, int n);\n",
        "__global__ void vector_transform(float *v, int n, float avg);\n",
        "__host__ bool check_result(float *v1, float* v2, float* v3, int n, float avg, float deviation);\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    for(int n = 1; n < 65432; n+=3){\n",
        "      float *v1, *v2, *v3, avg, deviation;\n",
        "    \n",
        "      initialize_vector(&v1, n);\n",
        "      initialize_vector(&v2, n);\n",
        "      v3 = (float*) malloc(sizeof(float) * n);\n",
        "\n",
        "      operate_on_GPU(v1, v2, v3, n, &avg, &deviation);\n",
        "\n",
        "      if(check_result(v1, v2, v3, n, avg, deviation))\n",
        "        printf(\"(%d) Correct!\\n\", n);\n",
        "      else\n",
        "        printf(\"(%d) False!\\n\", n);\n",
        "\n",
        "      free(v3);\n",
        "      free(v2);\n",
        "      free(v1);\n",
        "    }\n",
        " \n",
        "    return 0;\n",
        "}\n",
        "\n",
        "__host__ void initialize_vector(float** v, int n)\n",
        "{\n",
        "    *v = (float*) malloc(sizeof(float) * n);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      (*v)[i] = i;\n",
        "}\n",
        "\n",
        "__host__ void print_vector(const char* lbl, float* v, int n)\n",
        "{\n",
        "    printf(\"%s = |\\t\", lbl);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      printf(\"%f\\t\", v[i]);\n",
        "    printf(\"|\\n\");\n",
        "}\n",
        "\n",
        "__host__ void operate_on_GPU(float* v1, float* v2, float* v3, int n, float* avg, float* deviation)\n",
        "{\n",
        "    float* dev_v1, *dev_v2, *dev_v3, *dev_sum, *dev_sum_2;\n",
        "\n",
        "    int grid_size = min(256, (n / BLOCK_SIZE) + 1);\n",
        "    size_t full = sizeof(float) * n,\n",
        "            part = sizeof(float) * grid_size;\n",
        " \n",
        "    cudaMalloc(&dev_v1, full);\n",
        "    cudaMalloc(&dev_v2, full);\n",
        "    cudaMalloc(&dev_v3, full);\n",
        "    cudaMalloc(&dev_sum, part);\n",
        "    cudaMalloc(&dev_sum_2, sizeof(float));\n",
        " \n",
        "    cudaMemcpy(dev_v1, v1, full, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_v2, v2, full, cudaMemcpyHostToDevice);\n",
        " \n",
        "    vector_sum<<<grid_size, BLOCK_SIZE>>>(dev_v1, dev_v2, dev_v3, n);\n",
        " \n",
        "    cudaMemcpy(v3, dev_v3, full, cudaMemcpyDeviceToHost);\n",
        " \n",
        "    reduce_sum<<<grid_size, BLOCK_SIZE>>>(dev_v3, dev_sum, n);\n",
        "    reduce_sum<<<1, BLOCK_SIZE>>>(dev_sum, dev_sum_2, grid_size);\n",
        "\n",
        "    float temp;\n",
        "    cudaMemcpy(&temp, dev_sum_2, sizeof(float), cudaMemcpyDeviceToHost);\n",
        " \n",
        "    (*avg) = temp / n;\n",
        "\n",
        "    vector_transform<<<grid_size, BLOCK_SIZE>>>(dev_v3, n, *avg);\n",
        "    reduce_sum<<<grid_size, BLOCK_SIZE>>>(dev_v3, dev_sum, n);\n",
        "    reduce_sum<<<1, BLOCK_SIZE>>>(dev_sum, dev_sum_2, grid_size);\n",
        " \n",
        "    cudaMemcpy(deviation, dev_sum_2, sizeof(float), cudaMemcpyDeviceToHost);\n",
        " \n",
        "    *deviation = sqrt(1.0 / n * (*deviation));\n",
        " \n",
        "    cudaFree(dev_sum_2);\n",
        "    cudaFree(dev_sum);\n",
        "    cudaFree(dev_v3);\n",
        "    cudaFree(dev_v2);\n",
        "    cudaFree(dev_v1);\n",
        "}\n",
        "\n",
        "__global__ void vector_sum(float* v1, float* v2, float* v3, int n)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    while(tid < n) {\n",
        "        v3[tid] = v1[tid] + v2[tid];\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reduce_sum(float* v, float* sum, int n)\n",
        "{\n",
        "    int tid = 2 * blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    __shared__ float sh[BLOCK_SIZE];\n",
        "\n",
        "    sh[threadIdx.x] = 0;\n",
        " \n",
        "    while(tid < n) {\n",
        "        sh[threadIdx.x] += v[tid];\n",
        "        if(tid + blockDim.x < n)\n",
        "          sh[threadIdx.x] += v[tid + blockDim.x];\n",
        "        tid += 2 * blockDim.x * gridDim.x;\n",
        "    }\n",
        " \n",
        "    for(int i = blockDim.x >> 1; i > threadIdx.x; i >>= 1)\n",
        "    {\n",
        "      __syncthreads();\n",
        "      sh[threadIdx.x] += sh[threadIdx.x + i];\n",
        "    }\n",
        "\n",
        "    if(!threadIdx.x) {\n",
        "      sum[blockIdx.x] = sh[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void vector_transform(float *v, int n, float avg)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    while(tid < n) {\n",
        "        float temp = v[tid];\n",
        "        v[tid] = (temp - avg) * (temp - avg);\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__ bool check_result(float *v1, float* v2, float* v3, int n, float avg, float deviation)\n",
        "{\n",
        "    bool c = true;\n",
        " \n",
        "    float sum = 0,\n",
        "    *check_v3 = (float*) malloc(sizeof(float) * n);\n",
        " \n",
        "    for(int i = 0; c && i < n; i++)\n",
        "    {\n",
        "      c = v3[i] == (check_v3[i] = v1[i] + v2[i]);\n",
        "      sum += check_v3[i];\n",
        "    }\n",
        " \n",
        "    float check_avg = sum / n;\n",
        " \n",
        "    if(c = (fabs(check_avg - avg) <= 1))\n",
        "    {\n",
        "        float dev = 0;\n",
        "        for(int i = 0; i < n; i++)\n",
        "          dev += (check_v3[i] - avg) * (check_v3[i] - avg);\n",
        "        dev = sqrt(1.0 / n * dev);\n",
        "        c = fabs(dev - deviation) <= 1;\n",
        "    }\n",
        "    return c;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6uwgreRdMa6"
      },
      "outputs": [],
      "source": [
        "filepath = \"jun_2021.cu\"  #@param { type: \"string\" }\n",
        "compiled_filepath = \"jun_2021\"  #@param { type: \"string\" }\n",
        "\n",
        "!nvcc -arch=sm_37 -gencode=arch=compute_37,code=sm_37 $filepath -o $compiled_filepath\n",
        "\n",
        "argv = \"\" #@param [] { allow-input: true }\n",
        "\n",
        "!./$compiled_filepath $argv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGl3MRTLFWIc"
      },
      "source": [
        "# Kolokvijum 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY4y8GEpFdH7"
      },
      "outputs": [],
      "source": [
        "%%writefile kolokvijum_2022.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "#include <math.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__host__ void initialize_vector(int** A, int n);\n",
        "__host__ void operate_on_GPU(int* A, int* B, float*C, int n, float p);\n",
        "__global__ void kernel(int* A, int* B, float*C, int n, float p);\n",
        "__host__ bool check_result(int*A, int*B, float*C, int n, float p);\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    int *A, *B;\n",
        "    float p = .3, *C;\n",
        "    \n",
        "    for(int n = 255; n < 3245; n += 3)\n",
        "    { \n",
        "      initialize_vector(&A, n);\n",
        "      initialize_vector(&B, n);\n",
        "      C = (float*) malloc(sizeof(float) * (n - 2));\n",
        "\n",
        "      operate_on_GPU(A, B, C, n, p);\n",
        "\n",
        "      if(check_result(A, B, C, n, p))\n",
        "        cout << n << \" Correct!\" << endl;\n",
        "      else\n",
        "        cout << n << \" False!\" << endl;\n",
        "\n",
        "      free(C);\n",
        "      free(B);\n",
        "      free(A);\n",
        "    }\n",
        " \n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "__host__ void initialize_vector(int** A, int n)\n",
        "{\n",
        "    *A = (int*)malloc(sizeof(int) * n);\n",
        "    for(int i = 0; i < n; i++)\n",
        "      (*A)[i] = i + 1;//rand() % 100;\n",
        "}\n",
        "\n",
        "__host__ void operate_on_GPU(int* A, int* B, float* C, int n, float p)\n",
        "{\n",
        "    int* dev_A, *dev_B;\n",
        "    float *dev_C;\n",
        "    size_t full = sizeof(int) * n,\n",
        "            part = sizeof(float) * (n - 2);\n",
        "\n",
        "    cudaError_t err;\n",
        " \n",
        "    err = cudaMalloc(&dev_A, full);\n",
        "    if(err)\n",
        "      cout << \"1A \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMalloc(&dev_B, full);\n",
        "    if(err)\n",
        "          cout << \"1B \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMalloc(&dev_C, part);\n",
        "    if(err)\n",
        "          cout << \"1C \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMemcpy(dev_A, A, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2B \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    err = cudaMemcpy(dev_B, B, full, cudaMemcpyHostToDevice);\n",
        "    if(err)\n",
        "          cout << \"2B \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    kernel<<<min(256, (n / BLOCK_SIZE) + 1), BLOCK_SIZE>>>(dev_A, dev_B, dev_C, n, p);\n",
        "\n",
        "    err = cudaMemcpy(C, dev_C, part, cudaMemcpyDeviceToHost);\n",
        "    if(err)\n",
        "          cout << \"3 \" << cudaGetErrorString(err) << endl;\n",
        " \n",
        "    cudaFree(dev_C);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_A);\n",
        "}\n",
        "\n",
        "__global__ void kernel(int* A, int* B, float *C, int n, float p)\n",
        "{\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    __shared__ int shA[BLOCK_SIZE];\n",
        "    __shared__ int shB[BLOCK_SIZE];\n",
        "\n",
        "    while(tid < n)\n",
        "    {\n",
        "        shA[threadIdx.x] = A[tid];\n",
        "        shB[threadIdx.x] = B[tid];\n",
        "        __syncthreads();\n",
        "     \n",
        "        if(tid < n - 2)\n",
        "        {\n",
        "          if(threadIdx.x < blockDim.x - 2)\n",
        "            C[tid] = (shA[threadIdx.x] + shA[threadIdx.x + 1] + shA[threadIdx.x + 2]) * p + (shB[threadIdx.x] + shB[threadIdx.x + 1] + shB[threadIdx.x + 2]) * (1 - p);\n",
        "          else if(threadIdx.x < blockDim.x - 1)\n",
        "            C[tid] = (shA[threadIdx.x] + shA[threadIdx.x + 1] + A[tid + 1]) * p + (shB[threadIdx.x] + shB[threadIdx.x + 1] + B[tid + 1]) * (1 - p);\n",
        "          else\n",
        "            C[tid] = (shA[threadIdx.x] + A[tid + 1] + A[tid + 2]) * p + (shB[threadIdx.x] + B[tid + 1] + B[tid + 2]) * (1 - p);\n",
        "        }\n",
        "     \n",
        "        __syncthreads();\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\n",
        "__host__ bool check_result(int*A, int*B, float*C, int n, float p)\n",
        "{\n",
        "    bool c = true;\n",
        "    for(int i = 0; c && i < n - 2; i++)\n",
        "      c = (fabs(C[i] - ((A[i] + A[i + 1] + A[i + 2]) * p + (B[i] + B[i + 1] + B[i + 2]) * (1 - p))) <= 1);\n",
        "    return c;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdaOsxKwFhOa"
      },
      "outputs": [],
      "source": [
        "filepath = \"kolokvijum_2022.cu\"  #@param { type: \"string\" }\n",
        "compiled_filepath = \"kolokvijum_2022\"  #@param { type: \"string\" }\n",
        "\n",
        "!nvcc -arch=sm_37 -gencode=arch=compute_37,code=sm_37 $filepath -o $compiled_filepath\n",
        "\n",
        "argv = \"\" #@param [] { allow-input: true }\n",
        "\n",
        "!./$compiled_filepath $argv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9EAF1A2SqhMH",
        "RwMuu9sHqnW9",
        "ILZuf9T2cJSJ",
        "cGl3MRTLFWIc"
      ],
      "name": "Untitled2 (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}